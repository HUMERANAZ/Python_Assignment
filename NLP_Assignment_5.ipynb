{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d4b0507",
   "metadata": {},
   "source": [
    "# NLP_Assignment_5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507a3755",
   "metadata": {},
   "source": [
    "#### 1. What are Sequence-to-sequence models?\n",
    "\n",
    "Sequence-to-sequence (Seq2Seq) models are a type of neural network architecture that is designed to handle input and output sequences of different lengths. They consist of two main components: an encoder and a decoder. The encoder processes the input sequence and compresses it into a fixed-size context vector, also known as the latent representation or thought vector. The decoder takes the context vector as input and generates the output sequence, one step at a time, considering the previously generated tokens. Seq2Seq models are widely used in machine translation, text summarization, chatbot systems, and other tasks where the input and output are sequential data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 2.  What are the Problem with Vanilla RNNs?\n",
    "\n",
    "Vanilla RNNs (Recurrent Neural Networks) suffer from several issues:\n",
    "\n",
    "- Vanishing/Exploding Gradients: In long sequences, the gradients can either vanish or explode during backpropagation, leading to difficulties in learning long-term dependencies.\n",
    "\n",
    "- Short-Term Memory: Vanilla RNNs struggle to retain information from earlier time steps, limiting their ability to capture long-range dependencies in sequential data.\n",
    "\n",
    "- Lack of Contextual Understanding: Vanilla RNNs treat all previous inputs equally and do not have the ability to selectively focus on important information in the sequence.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 3. What is Gradient clipping?\n",
    "\n",
    "Gradient clipping is a technique used during the training of neural networks, including RNNs, to mitigate the problem of exploding gradients. It involves scaling down the gradients if their norm exceeds a predefined threshold. By capping the gradient values, gradient clipping helps stabilize the training process and prevents the model from diverging or getting stuck in large weight updates.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 4. Explain Attention mechanism\n",
    "\n",
    "Attention mechanism is a component used in Seq2Seq models that helps the model focus on specific parts of the input sequence while generating the output sequence. It assigns weights to different parts of the input sequence based on their relevance or importance for generating each output element. This allows the model to selectively attend to relevant information and capture long-range dependencies effectively. Attention mechanisms have greatly improved the performance of Seq2Seq models in tasks such as machine translation and text summarization.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 5. Explain Conditional random fields (CRFs)\n",
    "\n",
    "Conditional Random Fields (CRFs) are probabilistic models often used in sequence labeling tasks, such as named entity recognition or part-of-speech tagging. CRFs take into account the dependencies between adjacent labels in a sequence and model the conditional probabilities of different label sequences given the input. They consider both the observed features of the input sequence and the label dependencies, making them effective for sequence labeling tasks.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 6. Explain self-attention\n",
    "\n",
    "Self-attention, also known as intra-attention or scaled dot-product attention, is a mechanism used to capture dependencies between different positions within a sequence. Unlike traditional attention, self-attention allows a sequence to attend to other positions within itself. It computes attention scores by measuring the similarity between different positions and uses these scores to aggregate information from other positions. Self-attention has been successfully used in models such as the Transformer, enabling them to capture long-range dependencies in sequences effectively.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 7.  What is Bahdanau Attention?\n",
    "\n",
    "Bahdanau Attention, named after Dzmitry Bahdanau, is an attention mechanism used in Seq2Seq models. It allows the decoder to focus on different parts of the input sequence while generating each output element. Bahdanau Attention uses a trainable alignment model to calculate attention weights based on the decoder's hidden state and the encoder's output at each time step. These weights are used to compute a context vector that captures the relevant information from the input sequence. Bahdanau Attention has been widely used in machine translation models and has contributed to significant improvements in translation quality.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 8. What is a Language Model?\n",
    "\n",
    "A Language Model is a statistical model or neural network model that is trained on a large corpus of text and is capable of predicting the probability of a given sequence of words. Language models are designed to capture the statistical patterns and dependencies in natural language. They can be used to generate coherent text, perform language-related tasks\n",
    "\n",
    " such as machine translation or text completion, and evaluate the likelihood of a sentence being grammatically correct.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 9. What is Multi-Head Attention?\n",
    "\n",
    "Multi-Head Attention is an extension of the attention mechanism used in models like Transformer. It allows the model to attend to different parts of the input sequence simultaneously, using multiple sets of attention weights. Each attention head focuses on capturing different aspects or dependencies in the sequence. By employing multiple heads, the model can capture different types of information and enable better representation learning.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 10. What is Bilingual Evaluation Understudy (BLEU)\n",
    "\n",
    "Bilingual Evaluation Understudy (BLEU) is a metric used to evaluate the quality of machine-generated translations by comparing them to one or more reference translations. BLEU measures the n-gram overlap between the generated translation and the reference translations. It computes precision scores for different n-gram orders and combines them using a weighted average. BLEU scores range from 0 to 1, with higher scores indicating better translation quality. BLEU is widely used in machine translation research and evaluation to compare the performance of different translation systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d640eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
