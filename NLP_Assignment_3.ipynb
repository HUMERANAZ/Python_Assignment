{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "921c6eaa",
   "metadata": {},
   "source": [
    "# NLP_Assignment_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e3f7fe",
   "metadata": {},
   "source": [
    "#### 1. Explain the basic architecture of RNN cell.\n",
    "\n",
    "The basic architecture of a Recurrent Neural Network (RNN) cell consists of a single recurrent unit. The recurrent unit takes input from the current time step, the output of the previous time step, and an optional context vector. It performs computations on this input and produces an output and a hidden state. The hidden state is then passed as input to the next time step, creating a feedback loop that allows the RNN to process sequential data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 2. Explain Backpropagation through time (BPTT)\n",
    "\n",
    "\n",
    "Backpropagation through time (BPTT) is a technique used to train recurrent neural networks (RNNs). It is an extension of the backpropagation algorithm, which is used to train feedforward neural networks. BPTT involves unrolling the recurrent network across time steps and then applying the standard backpropagation algorithm to calculate gradients and update the model's parameters. It allows the RNN to learn from the entire sequence of inputs and outputs, considering the temporal dependencies.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 3. Explain Vanishing and exploding gradients\n",
    "\n",
    "Vanishing gradients and exploding gradients are issues that can occur during the training of deep neural networks, including recurrent neural networks.\n",
    "\n",
    "- Vanishing gradients: It refers to the problem where the gradients calculated during backpropagation become extremely small as they are propagated backward through many layers. As a result, the model has difficulty learning long-term dependencies. Vanishing gradients can hinder the training process and limit the RNN's ability to capture and retain information from earlier time steps.\n",
    "\n",
    "- Exploding gradients: It is the opposite problem, where the gradients during backpropagation become extremely large. This can lead to unstable training and cause the model's parameters to update dramatically, making the training process diverge. Exploding gradients can prevent the network from converging to a good solution.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 4. Explain Long short-term memory (LSTM)\n",
    "\n",
    "Long Short-Term Memory (LSTM) is a type of recurrent neural network architecture designed to address the vanishing gradient problem and capture long-term dependencies. It introduces memory cells and gates that regulate the flow of information within the network.\n",
    "\n",
    "LSTM cells have three main components: a memory cell, an input gate, and an output gate. The memory cell retains information over long sequences, while the gates control the flow of information into and out of the cell. This allows the LSTM to selectively remember or forget information based on the context.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 5. Explain Gated recurrent unit (GRU)\n",
    "\n",
    "Gated Recurrent Unit (GRU) is another type of recurrent neural network architecture that addresses the vanishing gradient problem and captures long-term dependencies. GRU combines the memory cell and input gate of LSTM into a single \"update gate\" and eliminates the output gate.\n",
    "\n",
    "GRU has two main components: an update gate and a reset gate. The update gate determines how much of the previous hidden state to retain and how much of the new input to incorporate. The reset gate controls how much of the previous hidden state should be ignored when computing the new hidden state.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 6. Explain Peephole LSTM\n",
    "\n",
    "Peephole LSTM is an extension of the standard LSTM architecture that introduces additional connections called \"peephole connections.\" Peephole connections allow the LSTM cell to access the memory cell state when making decisions about gating.\n",
    "\n",
    "In addition to the input gate, output gate, and forget gate, peephole LSTM includes connections that allow the gates to directly observe the memory cell state. This provides the gates with more information to make informed decisions about the flow of information within the cell.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 7. Bidirectional RNNs\n",
    "\n",
    "Bidirectional RNNs (BiRNNs) are a type of recurrent neural network that processes input sequences in both forward and backward directions. It combines two separate RNNs: one that processes the input sequence from the beginning to the end (forward RNN) and another that processes the sequence in reverse (backward RNN).\n",
    "\n",
    "BiRNNs are useful when capturing dependencies that depend on both past and future contexts. By processing the input sequence in both directions, the model can consider information from\n",
    "\n",
    " both earlier and later time steps simultaneously, enabling a more comprehensive understanding of the sequence.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 8. Explain the gates of LSTM with equations.\n",
    "\n",
    "The gates of an LSTM cell are responsible for controlling the flow of information. They are typically defined using sigmoid activation functions. The equations for the gates are as follows:\n",
    "\n",
    "- Input Gate (i): i(t) = sigmoid(W_i * [h(t-1), x(t)] + b_i)\n",
    "- Forget Gate (f): f(t) = sigmoid(W_f * [h(t-1), x(t)] + b_f)\n",
    "- Output Gate (o): o(t) = sigmoid(W_o * [h(t-1), x(t)] + b_o)\n",
    "\n",
    "In these equations, h(t-1) represents the previous hidden state, x(t) represents the current input, [h(t-1), x(t)] denotes the concatenation of the previous hidden state and current input, W_i, W_f, W_o are weight matrices, and b_i, b_f, b_o are bias vectors.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 9. Explain BiLSTM\n",
    "\n",
    "BiLSTM (Bidirectional LSTM) is a variant of LSTM that incorporates bidirectional processing. It consists of two LSTM layers, one processing the input sequence in the forward direction and the other in the backward direction.\n",
    "\n",
    "During training and inference, the outputs of both forward and backward LSTM layers are concatenated or combined in some way to generate the final output. BiLSTM is useful for tasks that require capturing dependencies from both past and future contexts, as it allows the model to consider information from both directions.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 10. Explain BiGRU\n",
    "\n",
    "BiGRU (Bidirectional Gated Recurrent Unit) is a variant of GRU that incorporates bidirectional processing. It follows the same principle as BiLSTM, with two separate GRU layers processing the input sequence in the forward and backward directions.\n",
    "\n",
    "Similar to BiLSTM, BiGRU captures dependencies from both past and future contexts by combining the outputs of the forward and backward GRU layers. It enables the model to have a comprehensive understanding of the input sequence and is beneficial for tasks that require bidirectional information processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfc4fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
