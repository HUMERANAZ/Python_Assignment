{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3729ca94",
   "metadata": {},
   "source": [
    "# NLP_Assignment_7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e7ee2b",
   "metadata": {},
   "source": [
    "#### 1. What is the architecture of BERT?\n",
    "\n",
    "The architecture of BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based model that revolutionized natural language processing tasks. BERT consists of a stack of transformer encoders. Here's a breakdown of its architecture:\n",
    "Input Representation: BERT takes tokenized input, where each word or subword is assigned a token. It adds special tokens like [CLS] at the beginning of the sequence and [SEP] between sentences.\n",
    "\n",
    "Token Embeddings: BERT uses token embeddings to represent the input tokens. These embeddings are learned during the pretraining phase and capture the meaning of individual tokens.\n",
    "\n",
    "Segment Embeddings: BERT incorporates segment embeddings to differentiate between sentences in tasks that require understanding of sentence-level relationships. It assigns a segment embedding of 0 or 1 to each token to indicate the sentence it belongs to.\n",
    "\n",
    "Positional Embeddings: BERT includes positional embeddings to convey the order of the tokens in the sequence. They help the model understand the sequential context.\n",
    "\n",
    "Transformer Encoder Stack: BERT utilizes a stack of transformer encoders. Each encoder has a multi-head self-attention mechanism and position-wise feed-forward neural networks. The self-attention mechanism allows the model to attend to different parts of the input sequence while considering their interdependencies.\n",
    "\n",
    "Pretraining and Fine-tuning: BERT is pretrained on a large corpus using unsupervised learning objectives, such as masked language modeling and next sentence prediction. After pretraining, the model can be fine-tuned on specific downstream tasks by adding task-specific layers and training on labeled data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 2. What is Masked Language Modeling (MLM)?\n",
    "\n",
    "Masked Language Modeling (MLM) is a pretraining objective used in BERT. In MLM, a certain percentage of the input tokens in a sequence are randomly masked. The objective is to predict the original value of the masked tokens based on the context of the surrounding tokens. By training the model to fill in the masked tokens, BERT learns to understand the bidirectional context and captures the relationships between different words in a sentence.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 3. What is Next Sentence Prediction (NSP)?\n",
    "\n",
    "Next Sentence Prediction (NSP) is another pretraining objective used in BERT. NSP is designed to enable BERT to understand sentence-level relationships, such as coherence and inference. During pretraining, BERT is trained to predict whether a given pair of sentences in a document appear consecutively or not. This task helps BERT learn to encode and reason about relationships between sentences, which is beneficial for downstream tasks that involve understanding the relationship between multiple sentences.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 4. What is Matthews evaluation?\n",
    "\n",
    "Matthews evaluation refers to the evaluation method based on the Matthews Correlation Coefficient (MCC). The MCC is a metric used to assess the performance of classification models, particularly in binary classification tasks. It takes into account true positives, true negatives, false positives, and false negatives to measure the quality of the predictions. The MCC ranges from -1 to 1, where 1 represents a perfect prediction, 0 indicates a random prediction, and -1 indicates a completely incorrect prediction.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 5. What is Matthews Correlation Coefficient (MCC)?\n",
    "\n",
    "The Matthews Correlation Coefficient (MCC) is a metric used to evaluate the performance of binary classification models. It takes into account true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) to measure the quality of the predictions. MCC ranges from -1 to 1, where 1 represents a perfect prediction, 0 indicates a random prediction, and -1 indicates a completely incorrect prediction. MCC is particularly useful when dealing with imbalanced datasets, as it considers the class distribution in the evaluation.\n",
    "\n",
    "\n",
    "\n",
    "#### 6. Explain Semantic Role Labeling.\n",
    "\n",
    "Semantic Role Labeling (SRL) is a natural language processing task that involves identifying and classifying the semantic roles of words or phrases in a sentence. The goal of SRL is to determine the roles that different constituents play in the predicate-argument structure of a sentence, such as the subject, object, or location. SRL provides a deeper understanding of the meaning and relationships between words in a sentence, which is useful in various applications like question-answering, information extraction, and machine translation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 7. Why does Fine-tuning a BERT model take less time than pretraining?\n",
    "Fine-tuning a BERT model takes less time than pretraining because pretraining involves training the model on a large corpus using unsupervised learning objectives, which requires a substantial amount of computational resources and time. During pretraining, BERT learns to understand language by predicting masked words and sentence relationships. On the other hand, fine-tuning utilizes the pretrained BERT model and trains it on specific downstream tasks with labeled data. Since fine-tuning focuses on adapting the pretrained model to a specific task, it typically requires less data and training time compared to the initial pretraining phase.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 8. What is Recognizing Textual Entailment (RTE)?\n",
    "\n",
    "Recognizing Textual Entailment (RTE) is a natural language processing task that involves determining whether a\n",
    "\n",
    " given hypothesis can be inferred or logically implied from a given text (premise). It is a form of text understanding that assesses the relationship between two pieces of text in terms of their logical entailment or contradiction. RTE is commonly used in applications such as question-answering, information retrieval, and text summarization.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 9. Explain the decoder stack of GPT models.\n",
    "\n",
    "The decoder stack of GPT (Generative Pretrained Transformer) models consists of multiple layers of transformer decoders. Each decoder layer has a self-attention mechanism and a feed-forward neural network. The self-attention mechanism allows the model to attend to different parts of the input sequence while considering their interdependencies. The decoder stack is responsible for generating the output sequence by progressively refining the representations and capturing the dependencies between tokens. It enables GPT models to generate coherent and contextually appropriate text, making them well-suited for tasks like language generation, text completion, and text summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e0ca32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
