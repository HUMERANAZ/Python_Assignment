{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d1d568b",
   "metadata": {},
   "source": [
    "# NLP_Assignment_6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90128199",
   "metadata": {},
   "source": [
    "#### 1. What are Vanilla autoencoders\n",
    "\n",
    "Vanilla autoencoders, also known as basic autoencoders, are a type of neural network architecture used for unsupervised learning and dimensionality reduction. They consist of an encoder network that compresses the input data into a lower-dimensional representation (latent space), and a decoder network that tries to reconstruct the original input from the compressed representation. The goal of a vanilla autoencoder is to minimize the reconstruction error, encouraging the model to learn a compact representation that captures the essential features of the input data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 2. What are Sparse autoencoders\n",
    "\n",
    "Sparse autoencoders are a variation of autoencoders that impose sparsity constraints on the latent space representation. In addition to minimizing the reconstruction error, sparse autoencoders also aim to activate only a small fraction of the neurons in the latent space. This encourages the model to learn more meaningful and disentangled representations, as it is forced to focus on the most relevant features of the input data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 3. What are Denoising autoencoders\n",
    "\n",
    "Denoising autoencoders are designed to handle noisy input data. They are trained to reconstruct the original, clean input by learning to remove the added noise. During training, the autoencoder is presented with corrupted versions of the input data and is trained to minimize the reconstruction error between the denoised output and the original, uncorrupted input. Denoising autoencoders can learn robust representations that are less affected by noise and can be used for denoising or removing noise from other data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 4. What are Convolutional autoencoders\n",
    "Convolutional autoencoders are a variant of autoencoders that employ convolutional neural network (CNN) layers in the encoder and decoder networks. These autoencoders are particularly effective for processing and reconstructing data with a grid-like structure, such as images. By utilizing convolutional layers, the model can capture spatial hierarchies and local patterns in the input data, resulting in more efficient and accurate reconstructions.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 5. What are Stacked autoencoders\n",
    "\n",
    "Stacked autoencoders, also known as deep autoencoders or multilayer autoencoders, are composed of multiple layers of encoders and decoders. Each layer learns to represent the input data at a different level of abstraction. The output of one layer serves as the input for the subsequent layer, forming a hierarchical structure. Stacked autoencoders can learn more complex representations and capture higher-level features by stacking multiple encoding and decoding layers together.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 6. Explain how to generate sentences using LSTM autoencoders\n",
    "\n",
    "LSTM autoencoders can be used to generate sentences by training the autoencoder to learn a compressed representation of text and then decoding it to generate new sentences. The process typically involves the following steps:\n",
    "   - Preprocess the text data, including tokenization, creating a vocabulary, and encoding the sentences into a numerical format.\n",
    "   - Design an LSTM-based autoencoder, where the encoder LSTM network compresses the input sentence into a fixed-length representation (latent space), and the decoder LSTM network generates a sentence from the latent representation.\n",
    "   - Train the autoencoder using a dataset of sentences, optimizing the reconstruction loss, which measures the dissimilarity between the original and reconstructed sentences.\n",
    "   - To generate new sentences, input a random or partial sentence to the encoder, obtain the compressed representation, and use the decoder LSTM to generate the corresponding sentence by decoding the representation. This process can be repeated to generate multiple sentences.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 7. Explain Extractive summarization\n",
    "\n",
    "Extractive summarization is a technique used to create a summary of a document by selecting and combining important sentences or phrases from the original text. It involves identifying the most relevant sentences that capture the key information or meaning of the document. Extractive summarization methods typically employ algorithms that rank sentences based on their importance, using features like sentence length, word frequency, or the presence of specific keywords. The highest-ranked sentences are then selected and concatenated to form the summary, often preserving the original order of the sentences.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 8.  Explain Abstractive summarization\n",
    "\n",
    "Abstractive summarization is a technique used to generate a\n",
    "\n",
    " summary of a document by understanding the content and generating new sentences that convey the essential information. Unlike extractive summarization, abstractive methods do not rely solely on selecting sentences from the original text. Instead, they use natural language generation techniques, such as deep learning models like sequence-to-sequence models with attention mechanisms or transformer models like BERT, to generate coherent and concise summaries in a more human-like manner. Abstractive summarization allows for more flexibility and the generation of summaries that may not be present verbatim in the source document.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 9. Explain Beam search\n",
    "\n",
    "Beam search is a search algorithm commonly used in sequence generation tasks, such as machine translation or text generation with neural networks. It is an extension of the greedy search strategy. Instead of selecting the most likely next word at each step, beam search keeps track of a fixed number (beam width) of the most probable sequences so far. It explores multiple paths simultaneously, expanding the search space. At each step, the algorithm considers all possible next words for each sequence, computes their probabilities, and keeps the top-k sequences with the highest cumulative probabilities. This process continues until a maximum length or a termination condition is reached. Beam search helps to mitigate the risk of getting stuck in local optima and allows for exploring diverse options during sequence generation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 10.  Explain Length normalization\n",
    "\n",
    "Length normalization is a technique used in sequence generation tasks, such as machine translation or text summarization, to address the issue of favoring shorter sequences over longer ones. Since shorter sequences often have higher probabilities, models biased towards shorter outputs may generate overly concise or incomplete results. Length normalization is applied to the probabilities of the generated sequences to penalize shorter sequences and prevent the bias. Commonly, it involves dividing the log-probability of a sequence by its length, encouraging the model to generate longer sequences with more balanced probabilities.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 11.  Explain Coverage normalization\n",
    "\n",
    "Coverage normalization is a technique used in abstractive summarization to address the problem of repetitive or incoherent generated summaries. In the context of summarization, coverage refers to the idea of tracking the attention or importance given to different parts of the source document during the generation process. Coverage normalization helps to encourage the model to attend to different parts of the source document and avoid repeating or neglecting important information. It involves incorporating a coverage loss term during training that measures the difference between the attention distribution over the source document and the current generation step. By minimizing this loss, the model learns to generate summaries that cover the important aspects of the source text more evenly.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 12.  Explain ROUGE metric evaluation\n",
    "\n",
    "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of evaluation metrics commonly used for assessing the quality of automatic summaries. ROUGE measures the overlap between the generated summary and one or more reference summaries. It calculates various metrics based on the counts of n-gram overlaps, such as ROUGE-N (measuring unigram, bigram, trigram, etc., overlap), ROUGE-L (measuring longest common subsequence), ROUGE-S (measuring skip-bigram overlap), and others. These metrics provide a quantitative assessment of the quality, coverage, and coherence of the generated summaries compared to the reference summaries. ROUGE scores are widely used in research and evaluation of summarization systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316eecea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
